# Template out the image version and service account name to use so that they can be modified from Terraform.
image:
  tag: ${image_version}

serviceAccount:
  name: ${service_account_name}

# Tolerate all taints so that all nodes are logged.
tolerations:
  - key: ""
    operator: "Exists"
    effect: "NoExecute"

# Pipeline configuration.
config:
  # This is a config for the FluentBit application running the pipeline.
  # We set the flush interval, log level and where our parsers are stored.
  # We additionally expose the HTTP service so that Terraform can ping it.
  service: |
    [SERVICE]
        Daemon Off
        Flush 1
        Log_Level info
        Parsers_File parsers.conf
        HTTP_Server On
        HTTP_Listen 0.0.0.0
        HTTP_Port 2020
        Health_Check On
  # We read all of the logs for output by docker's json file logging engine.
  # Every line appended to a file that matches the Path field will generate a record in the pipeline.
  inputs: |
    [INPUT]
        Name                tail
        Tag                 kube.*
        Path                /var/log/containers/*.log
        # Parser              cri
        Mem_Buf_Limit       5MB
        Skip_Long_Lines     On
  # Grep Filter drops logs that are only whitespace.
  # Kubernetes Filter appends K8s information to all outgoing logs.
  filters: |
    # Dispose of logs that are empty
    [FILTER]
        Name                grep
        Match               kube.*
        Exclude             $message /^\s+$/

    # Dispose of logs for liveness/readiness check
    [FILTER]
        Name                grep
        Match               kube.*
        Exclude             $message /.*GIN-debug*./
        Exclude             $message /.*[GIN]*./

    [FILTER]
        Name                kubernetes
        Match               kube.*
        K8S-Logging.Parser  On
        K8S-Logging.Exclude On

    # Include logs for configured namespace
    [FILTER]
        Name                grep
        Match               kube.*
        Regex               $kubernetes['namespace_name'] ${namespaces}

    [FILTER]
        Name                lua
        Match               kube.*
        call                set_topic
        code                function set_topic(tag, timestamp, record) if record["kubernetes"]["namespace_name"] ~= nil then record["namespace"] = "logs_" .. record["kubernetes"]["namespace_name"] end return 2, timestamp, record end

  # We output the logs coming out of the Kubernetes Filter to Cloudwatch.
  outputs: |
    [OUTPUT]
        Name                kafka
        Match               kube.*
        Brokers             ${kafka_brokers}
        topics              logs_kubernetes
        topic_key           namespace
        dynamic_topic       On
        Timestamp_Key       @timestamp
        Retry_Limit         false
        # hides errors "Receive failed: Disconnected" when kafka kills idle connections
        rdkafka.log.connection.close          false
        # producer buffer is not included in http://fluentbit.io/documentation/0.12/configuration/memory_usage.html#estimating
        rdkafka.queue.buffering.max.kbytes    10240
        # for logs you'll probably want this ot be 0 or 1, not more
        rdkafka.request.required.acks         1
